{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abce4d72",
   "metadata": {
    "id": "abce4d72",
    "ExecuteTime": {
     "end_time": "2025-06-11T05:22:04.362254900Z",
     "start_time": "2025-06-11T05:22:02.651442400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[dataset specification](https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_iris.html)\n",
    "## IRIS Dataset\n",
    "**class:** [setosa, versicolour, virginica]\n",
    "**Number of Instances**: `150` (50 in each of three classes)\n",
    "**Number of Attributes**: `4` numeric, predictive attributes and the class\n",
    "**Attribute Information**: `sepal length`, `sepal width`, `petal length`, `petal width`"
   ],
   "metadata": {
    "collapsed": false,
    "id": "238aaaf4d59584a1"
   },
   "id": "238aaaf4d59584a1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "iris_set = load_iris()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-11T05:22:04.371399400Z",
     "start_time": "2025-06-11T05:22:04.358063Z"
    }
   },
   "id": "22dafd4a5d641b18"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "  def __init__(self, dataset, indices, transform=None, encoder=None):\n",
    "    self.dataset, self.indices = dataset, indices\n",
    "    self.transform, self.encoder = transform, encoder\n",
    "  def __getitem__(self, item: int):\n",
    "    idx = self.indices[item]\n",
    "    feature, label = self.dataset.data[idx], self.dataset.target[idx]\n",
    "    if self.transform: feature = self.transform(feature)\n",
    "    if self.encoder: label = self.encoder(label)\n",
    "    return feature, label\n",
    "  def __len__(self): return len(self.indices)"
   ],
   "metadata": {
    "id": "82417d6f3c383b4b",
    "ExecuteTime": {
     "end_time": "2025-06-11T05:22:04.371399400Z",
     "start_time": "2025-06-11T05:22:04.367784900Z"
    }
   },
   "id": "82417d6f3c383b4b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "indices = random.sample(range(iris_set.data.__len__()), 100)\n",
    "\n",
    "# init Datasets\n",
    "trainset = Dataset(iris_set, indices[:50])\n",
    "testset = Dataset(iris_set, indices[50:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-11T05:22:04.380367900Z",
     "start_time": "2025-06-11T05:22:04.374925Z"
    }
   },
   "id": "86d5e6eb33670383"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression\n",
    "Linear regression is a type of modeling that shows the relationship between explanatory variables and scalar responses. It uses a linear approach called a \"linear model\". The algorithms that predict parameters must follow a key restriction: their conditional average must be expressed as an affine function. The most common algorithms for linear regression are least squares and Newton's method.\n",
    "In situations where the algorithm does not properly fit the model, we call it \"LOF, Lack of Fitting”, which has led to many optimization techniques and research."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9546bd8f3525741"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Variants of Linear Regression\n",
    "Linear regression models fall into two distinct categories based on their purpose:\n",
    "- If the model is used for understanding and analyzing the relationship between explanatory variables and dependent variables, it is called regression analysis.\n",
    "- If the model is used for prediction and forecasting, it is called a predictive model.\n",
    "\n",
    "These models can also be classified by their mathematical attributes:\n",
    "- Simple Linear Regression: A model with a single explanatory variable.\n",
    "- Multiple Linear Regression: A model with two or more explanatory variables.\n",
    "- Multivariate Linear Regression: A model with multiple dependent variables.\n",
    "\n",
    "**Loss Function Definition:**\n",
    "* $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "* $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b72373f0070a87b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc074530",
   "metadata": {
    "id": "fc074530",
    "ExecuteTime": {
     "end_time": "2025-06-11T05:22:04.398800300Z",
     "start_time": "2025-06-11T05:22:04.380868300Z"
    }
   },
   "outputs": [],
   "source": [
    "# define loss functions(MSE, MAE)\n",
    "def mean_squared_error(independent, dependent, weight):\n",
    "  probability = np.dot(independent, weight)\n",
    "  return np.mean((probability - dependent) ** 2)\n",
    "# mean_squared_error\n",
    "\n",
    "def mean_absolute_error(independent, dependent, weight):\n",
    "  probability = np.dot(independent, weight)\n",
    "  return np.mean(abs(probability - dependent))\n",
    "# mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Formulation\n",
    "$$y_i = \\beta_{0} + \\beta_{1} \\cdot x_{(i,1)} + ... + \\beta_{p} \\cdot x_{(i, p)} + \\epsilon_i = \\beta_p \\cdot x_{(i,p)} + \\epsilon_i$$\n",
    "- $x, y$ represents a vector of observations, which can be a multi-dimensional matrix.\n",
    "- $\\beta$ represents the model parameters, which have a dimension of $p + 1$.\n",
    "- $\\epsilon$ represents possible error."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c79074041fa9453"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# define LinearRegression\n",
    "class LinearRegression:\n",
    "  def __init__(self, n_inpt): self.weight = np.zeros(shape=(n_inpt))\n",
    "  def gdr(self, x, y, lr):\n",
    "    indications = self.forward(x)\n",
    "    self.weight -= (lr / x.shape[0]) * np.dot(x.T, (indications - y))\n",
    "  # gdr\n",
    "  def train(self, dataset, iters: int, lr=0.01):\n",
    "    for _ in range(iters):\n",
    "      for feature, label in dataset: self.gdr(feature, label, lr=lr)\n",
    "  # train\n",
    "  def forward(self, x): return np.dot(x, self.weight)\n",
    "# LogisticRegression"
   ],
   "metadata": {
    "id": "338d3230bab8351d",
    "ExecuteTime": {
     "end_time": "2025-06-11T05:22:04.399305500Z",
     "start_time": "2025-06-11T05:22:04.388632200Z"
    }
   },
   "id": "338d3230bab8351d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Understanding Learning Rule to Fit the Model using GDR, Gradient Descent Rule\n",
    "\n",
    "**GDR (Gradient Descent Rule)** is a learning rule and optimization technique for linear regression that helps fit the model to the problem. It minimizes the **Cost Function** by updating weights. This approach has become the fundamental workflow for optimization in modern machine learning and deep learning.\n",
    "\n",
    "- Initialize weight $\\theta$ as $0$ or random number.\n",
    "- Calculate the relationship between the model and real-world observations using cost function $J(\\theta)$.\n",
    "- Until $J(\\theta)$ is fully minimized, the algorithm continues calculating $w' = w - \\alpha \\cdot \\nabla{J(w)}$, where $w'$ is the newly updated weight and $w$ is the previous weight."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd5a6f194c013ad0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def GDR(model, lr):\n",
    "  def _GDR(x, y):\n",
    "    pred = model.forward(x)\n",
    "    model.weight -= lr * np.dot(x.T, (pred - y))\n",
    "  return _GDR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-11T05:22:04.399305500Z",
     "start_time": "2025-06-11T05:22:04.393308600Z"
    }
   },
   "id": "6446a779e7b08cc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 424.43it/s, loss=0.0818]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(10))\n",
    "\n",
    "# init and train a model\n",
    "model = LinearRegression(4)\n",
    "optimizer = GDR(model, 0.001)\n",
    "for _ in progress_bar:\n",
    "  loss = 0.\n",
    "  for feature, label in trainset:\n",
    "    optimizer(feature, label)\n",
    "    loss += mean_squared_error(feature, label, model.weight)\n",
    "  progress_bar.set_postfix(loss=loss/len(trainset))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c35f6a8",
    "outputId": "c9a422f9-7705-409e-ec84-67856eb3f80e",
    "ExecuteTime": {
     "end_time": "2025-06-11T05:22:31.789859200Z",
     "start_time": "2025-06-11T05:22:31.754926500Z"
    }
   },
   "id": "6c35f6a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Key Concepts and Limitations\n",
    "\n",
    "While deep learning and other advanced machine learning methods have largely superseded linear regression, it remains more cost-effective in certain cases.\n",
    "\n",
    "- **Exogeneity** is a measurement or property that is not related to the model's error.\n",
    "    - **Strict Exogeneity:** The model maintains exogeneity over an extended period.\n",
    "    - **Weak Exogeneity:** The model only maintains exogeneity over the current period.\n",
    "    - **Deterministic:** The model maintains exogeneity for past periods but not for current and future periods\n",
    "- **Linearity** means the relationship between parameters and explanatory variables can be measured through linear combinations.\n",
    "- **Constant Variance** means the model's error range remains independent of the predicted value. For example, if the model predicts an individual's income as 1000, their actual income might range from `800~1200`.\n",
    "    - **Independence of Errors** means that errors are not correlated with each other. This is one of the major limitations of linear regression, though it can be addressed through data regularization or Bayesian linear regression."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "726f4437563cbe23"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.88(44/50)\n"
     ]
    }
   ],
   "source": [
    "count, n_samples = 0, len(testset)\n",
    "for feature, label in testset:\n",
    "  pred = model.forward(feature)\n",
    "  if round(pred) == label: count += 1\n",
    "print(f\"accuracy: {count / n_samples:.2f}({count}/{n_samples})\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-11T05:22:56.332544600Z",
     "start_time": "2025-06-11T05:22:56.322237600Z"
    }
   },
   "id": "9f49ae95842b6c18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
