{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "abce4d72",
   "metadata": {
    "id": "abce4d72",
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:03.002736900Z",
     "start_time": "2025-03-02T03:46:02.949037400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "iris_set = load_iris()"
   ],
   "metadata": {
    "id": "22dafd4a5d641b18",
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:03.104328200Z",
     "start_time": "2025-03-02T03:46:02.961554200Z"
    }
   },
   "id": "22dafd4a5d641b18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[dataset specification](https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_iris.html)\n",
    "### Predict type of Iris by given Features\n",
    "**class**: [setosa, versicolour, virginica]\n",
    "**Number of Instances**: `150` (50 in each of three classes)\n",
    "**Number of Attributes**: `4` numeric, predictive attributes and the class\n",
    "**Attribute Information**:\n",
    "* sepal length in `cm`\n",
    "* sepal width in `cm`\n",
    "* petal length in `cm`\n",
    "* petal width in `cm`"
   ],
   "metadata": {
    "collapsed": false,
    "id": "238aaaf4d59584a1"
   },
   "id": "238aaaf4d59584a1"
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "  def __init__(self, dataset, indices, transform=None, encoder=None):\n",
    "    self.dataset, self.indices = dataset, indices\n",
    "    self.transform, self.encoder = transform, encoder\n",
    "  def __getitem__(self, item: int):\n",
    "    idx = self.indices[item]\n",
    "    feature, label = self.dataset.data[idx], self.dataset.target[idx]\n",
    "    if self.transform: feature = self.transform(feature)\n",
    "    if self.encoder: label = self.encoder(label)\n",
    "    return feature, label\n",
    "  def __len__(self): return len(self.indices)"
   ],
   "metadata": {
    "id": "82417d6f3c383b4b",
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:03.108866400Z",
     "start_time": "2025-03-02T03:46:02.990552700Z"
    }
   },
   "id": "82417d6f3c383b4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss Functions: MSE and MAE\n",
    "\n",
    "Loss functions are a critical component in machine learning models, particularly for regression tasks. They quantify the difference between the predicted values and the actual target values, guiding the optimization process to minimize this error.\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "MSE calculates the average of the squared differences between the predicted and actual values. It is defined as:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "- **Penalizes larger errors more heavily**: Squaring the errors makes MSE more sensitive to outliers.\n",
    "- **Continuous optimization**: Smooth gradients make it suitable for many gradient-based optimization algorithms.\n",
    "- **Use Case**: Ideal when large errors need to be penalized significantly.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "MAE calculates the average of the absolute differences between the predicted and actual values. It is defined as:\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "- **Treats all errors equally**: MAE is less sensitive to outliers compared to MSE.\n",
    "- **Robust to Outliers**: Does not disproportionately penalize larger deviations.\n",
    "- **Use Case**: Ideal for datasets where outliers are present and need to be treated equally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84067821d3244e27"
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fc074530",
   "metadata": {
    "id": "fc074530",
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:03.111347400Z",
     "start_time": "2025-03-02T03:46:03.002736900Z"
    }
   },
   "outputs": [],
   "source": [
    "# define loss functions(MSE, MAE)\n",
    "def mean_squared_error(independent, dependent, weight):\n",
    "  probability = np.dot(independent, weight)\n",
    "  return np.mean((probability - dependent) ** 2)\n",
    "# mean_squared_error\n",
    "\n",
    "def mean_absolute_error(independent, dependent, weight):\n",
    "  probability = np.dot(independent, weight)\n",
    "  return np.mean(abs(probability - dependent))\n",
    "# mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Descent with Regularization (GDR)\n",
    "Gradient Descent with Regularization (GDR) is an optimization technique used to minimize the loss function while controlling the complexity of the model. By incorporating regularization terms into the loss function, GDR helps to prevent overfitting and improves the generalization of the model to unseen data. Update rule:\n",
    "$$\\theta = \\theta - \\eta \\cdot \\nabla L(\\theta)$$\n",
    "Gradient Descent is an iterative optimization algorithm used to minimize the loss function by updating model parameters in the direction of the steepest descent of the gradient."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe2d8f9d88c01033"
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "# define LinearRegression\n",
    "class LinearRegression:\n",
    "  def __init__(self, n_inpt): self.weight = np.zeros(shape=(n_inpt))\n",
    "  def gdr(self, x, y, lr):\n",
    "    indications = self.forward(x)\n",
    "    self.weight -= (lr / x.shape[0]) * np.dot(x.T, (indications - y))\n",
    "  # gdr\n",
    "  def train(self, dataset, iters: int, lr=0.01):\n",
    "    for _ in range(iters):\n",
    "      for feature, label in dataset: self.gdr(feature, label, lr=lr)\n",
    "  # train\n",
    "  def forward(self, x): return np.dot(x, self.weight)\n",
    "# LogisticRegression"
   ],
   "metadata": {
    "id": "338d3230bab8351d",
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:03.112360800Z",
     "start_time": "2025-03-02T03:46:03.022915600Z"
    }
   },
   "id": "338d3230bab8351d"
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [],
   "source": [
    "def GDR(model, lr):\n",
    "  def _GDR(x, y):\n",
    "    pred = model.forward(x)\n",
    "    model.weight -= lr * np.dot(x.T, (pred - y))\n",
    "  return _GDR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:03.112360800Z",
     "start_time": "2025-03-02T03:46:03.033551800Z"
    }
   },
   "id": "6446a779e7b08cc"
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 182.31it/s, loss=0.0912]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(10))\n",
    "\n",
    "# init and train a model\n",
    "model = LinearRegression(4)\n",
    "optimizer = GDR(model, 0.001)\n",
    "for _ in progress_bar:\n",
    "  loss = 0.\n",
    "  for feature, label in support_set:\n",
    "    optimizer(feature, label)\n",
    "    loss += mean_squared_error(feature, label, model.weight)\n",
    "  progress_bar.set_postfix(loss=loss/len(support_set))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c35f6a8",
    "outputId": "c9a422f9-7705-409e-ec84-67856eb3f80e",
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:03.143296800Z",
     "start_time": "2025-03-02T03:46:03.048171900Z"
    }
   },
   "id": "6c35f6a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ordinal Linear Regression\n",
    "[wikipedia](https://en.wikipedia.org/wiki/Ordinal_regression)\n",
    "Ordinal Linear Regression is a statistical model used to predict an ordinal variable based on a given feature vector. The model outputs a continuous value (a float) within specific ranges corresponding to the ordinal categories. These predicted values are then mapped to discrete ordinal labels to classify instances appropriately.\n",
    "* **around 0 or below:** represents setosa\n",
    "* **around 1:** represents versicolour\n",
    "* **around 3 or above:** represents virginica"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3dfb6f8529519cc"
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.92(46/50)\n"
     ]
    }
   ],
   "source": [
    "count, n_samples = 0, len(query_set)\n",
    "for feature, label in support_set:\n",
    "  pred = model.forward(feature)\n",
    "  if round(pred) == label: count += 1\n",
    "print(f\"accuracy: {count / n_samples:.2f}({count}/{n_samples})\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:03.173605600Z",
     "start_time": "2025-03-02T03:46:03.116923300Z"
    }
   },
   "id": "9f49ae95842b6c18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
