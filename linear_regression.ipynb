{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "abce4d72",
   "metadata": {
    "id": "abce4d72",
    "ExecuteTime": {
     "end_time": "2025-03-02T05:22:43.704693600Z",
     "start_time": "2025-03-02T05:22:43.649829300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "iris_set = load_iris()"
   ],
   "metadata": {
    "id": "22dafd4a5d641b18",
    "ExecuteTime": {
     "end_time": "2025-03-02T05:22:43.706708300Z",
     "start_time": "2025-03-02T05:22:43.655741800Z"
    }
   },
   "id": "22dafd4a5d641b18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[dataset specification](https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_iris.html)\n",
    "## IRIS Dataset\n",
    "**class:** [setosa, versicolour, virginica]\n",
    "**Number of Instances**: `150` (50 in each of three classes)\n",
    "**Number of Attributes**: `4` numeric, predictive attributes and the class\n",
    "**Attribute Information**: `sepal length`, `sepal width`, `petal length`, `petal width`"
   ],
   "metadata": {
    "collapsed": false,
    "id": "238aaaf4d59584a1"
   },
   "id": "238aaaf4d59584a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 선형 회귀분석\n",
    "선형 회귀분석(Linear Regression)란 하나 이상의 설명변수와 그에 대한 스칼라 반응(sclar response)을 통한 모델링입니다. 그 중에서도 선형적인 접근을 하는 것을 지칭합니다.\n",
    "* 자료로 예측된 매개변수, 그로 만들어진 선형 예측자 함수(linear predictor function)로 입출력 관계를 모사합니다—이를 선형모델(linear model)이라 합니다. 대부분의 경우에 매개변수를 구하기 위한 알고리즘을 위해 설명변수와 모델 간의 반응과 그 조건부 평균은 결국 아핀 함수로 나타낼 수 있어야 합니다.\n",
    "* 일반적으로 최소 제곱법(least square)이 적합을 위해 사용됩니다. 이때, 적합이 제대로 작동하지 않은 상태를 적합성 결여(lack of fitting, LOF)이라 합니다—이를 피하기 위해 최소 제곱법에서 파생된 다양한 접근이 존재합니다.\n",
    "* 선형회귀는 결합부 확률 분포(joint probability distribution)보다는 조건부 확률 분포(conditional probability distribution)에 초점을 가지고 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9546bd8f3525741"
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "  def __init__(self, dataset, indices, transform=None, encoder=None):\n",
    "    self.dataset, self.indices = dataset, indices\n",
    "    self.transform, self.encoder = transform, encoder\n",
    "  def __getitem__(self, item: int):\n",
    "    idx = self.indices[item]\n",
    "    feature, label = self.dataset.data[idx], self.dataset.target[idx]\n",
    "    if self.transform: feature = self.transform(feature)\n",
    "    if self.encoder: label = self.encoder(label)\n",
    "    return feature, label\n",
    "  def __len__(self): return len(self.indices)"
   ],
   "metadata": {
    "id": "82417d6f3c383b4b",
    "ExecuteTime": {
     "end_time": "2025-03-02T05:22:43.706708300Z",
     "start_time": "2025-03-02T05:22:43.666281900Z"
    }
   },
   "id": "82417d6f3c383b4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 손실함수 정의하기\n",
    "손실함수(loss function)는 기계학습에서 모델의 예측값과 실제값 사이의 차이를 측정하는 함수입니다. 손실 함수는 모델이 얼마나 잘 수행되고 있는지를 수치적으로 나타내며, 이 값을 최소화하는 것이 모델 학습의 목표입니다. 개별적인 관찰에 대한 손실은 비용함수(cost function)라고 별칭합니다.\n",
    "* $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "* $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84067821d3244e27"
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "fc074530",
   "metadata": {
    "id": "fc074530",
    "ExecuteTime": {
     "end_time": "2025-03-02T05:22:43.722407Z",
     "start_time": "2025-03-02T05:22:43.670722500Z"
    }
   },
   "outputs": [],
   "source": [
    "# define loss functions(MSE, MAE)\n",
    "def mean_squared_error(independent, dependent, weight):\n",
    "  probability = np.dot(independent, weight)\n",
    "  return np.mean((probability - dependent) ** 2)\n",
    "# mean_squared_error\n",
    "\n",
    "def mean_absolute_error(independent, dependent, weight):\n",
    "  probability = np.dot(independent, weight)\n",
    "  return np.mean(abs(probability - dependent))\n",
    "# mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 수식화\n",
    "$\\left\\{ y_i, x_{i1}, …, x_{ip} \\right\\}^n_{i=1}$를 통해 선형모델은 독립변수 y와 종속변수 $\\beta$를 예측해야 합니다.\n",
    "$$y_{i} = \\beta_{0} + \\beta_{1}x_{i1} + ... + \\beta_{p}x_{ip} + \\epsilon_{i} = \\beta_{p}x_{ip} + \\epsilon_{i}$$\n",
    "* $y$는 관측값의 벡터입니다.\n",
    "* $x$는 열 벡터 $x_i$의 행렬 또는 다차원의 행 벡터 $x_j$입니다.\n",
    "* $\\beta$는 $p+1$ 차원의 매개변수 벡터입니다.\n",
    "* $\\epsilon$는 $\\epsilon_i$의 벡터입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe2d8f9d88c01033"
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "# define LinearRegression\n",
    "class LinearRegression:\n",
    "  def __init__(self, n_inpt): self.weight = np.zeros(shape=(n_inpt))\n",
    "  def gdr(self, x, y, lr):\n",
    "    indications = self.forward(x)\n",
    "    self.weight -= (lr / x.shape[0]) * np.dot(x.T, (indications - y))\n",
    "  # gdr\n",
    "  def train(self, dataset, iters: int, lr=0.01):\n",
    "    for _ in range(iters):\n",
    "      for feature, label in dataset: self.gdr(feature, label, lr=lr)\n",
    "  # train\n",
    "  def forward(self, x): return np.dot(x, self.weight)\n",
    "# LogisticRegression"
   ],
   "metadata": {
    "id": "338d3230bab8351d",
    "ExecuteTime": {
     "end_time": "2025-03-02T05:22:43.722407Z",
     "start_time": "2025-03-02T05:22:43.677508100Z"
    }
   },
   "id": "338d3230bab8351d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 경사 하강법\n",
    "경사하강법(Gradient Descent)은 머신러닝과 최적화 분야에서 널리 사용되는 알고리즘으로, 함수의 최솟값을 찾기 위해 반복적으로 파라미터를 업데이트하는 방법입니다. 주로 모델의 손실을 최소화하는 데 사용되며, 이 과정에서 모델의 가중치를 조정합니다.\n",
    "1. 가중치 $\\theta$를 임의의 값을 초기화합니다.\n",
    "2. 현재의 가중치 $\\theta$에서 손실함수의 경사 $\\nabla{f(\\theta)}$를 계산합니다.\n",
    "3. 가중치 갱신하기: $\\theta_{\\text{new}} \\leftarrow \\theta - \\mathcal{n}\\cdot\\nabla{f(\\theta)}$\n",
    "4. 경사가 충분히 작아질 때까지 2 ~ 3을 반복합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd5a6f194c013ad0"
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "def GDR(model, lr):\n",
    "  def _GDR(x, y):\n",
    "    pred = model.forward(x)\n",
    "    model.weight -= lr * np.dot(x.T, (pred - y))\n",
    "  return _GDR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T05:22:43.722407Z",
     "start_time": "2025-03-02T05:22:43.680837900Z"
    }
   },
   "id": "6446a779e7b08cc"
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 480.38it/s, loss=0.0912]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(10))\n",
    "\n",
    "# init and train a model\n",
    "model = LinearRegression(4)\n",
    "optimizer = GDR(model, 0.001)\n",
    "for _ in progress_bar:\n",
    "  loss = 0.\n",
    "  for feature, label in support_set:\n",
    "    optimizer(feature, label)\n",
    "    loss += mean_squared_error(feature, label, model.weight)\n",
    "  progress_bar.set_postfix(loss=loss/len(support_set))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c35f6a8",
    "outputId": "c9a422f9-7705-409e-ec84-67856eb3f80e",
    "ExecuteTime": {
     "end_time": "2025-03-02T05:22:43.725416100Z",
     "start_time": "2025-03-02T05:22:43.689139500Z"
    }
   },
   "id": "6c35f6a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 검증하기\n",
    "아래 검증 결과를 통해 해당 구현이 `0.92`의 정확도를 달성했음을 알 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65c600f1e1ed1b6b"
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.92(46/50)\n"
     ]
    }
   ],
   "source": [
    "count, n_samples = 0, len(query_set)\n",
    "for feature, label in support_set:\n",
    "  pred = model.forward(feature)\n",
    "  if round(pred) == label: count += 1\n",
    "print(f\"accuracy: {count / n_samples:.2f}({count}/{n_samples})\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T05:22:43.725416100Z",
     "start_time": "2025-03-02T05:22:43.717784800Z"
    }
   },
   "id": "9f49ae95842b6c18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
