{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62db0eaa",
   "metadata": {
    "id": "62db0eaa",
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:08.962502400Z",
     "start_time": "2025-03-02T03:46:08.911565400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import plotly.express as px\n",
    "from plotly.offline import iplot\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e66a9cf95b568f9a",
   "metadata": {
    "id": "e66a9cf95b568f9a",
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:09.080194300Z",
     "start_time": "2025-03-02T03:46:08.920833Z"
    }
   },
   "outputs": [],
   "source": [
    "cancer_set = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[data sepecification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "a6c2ddd45323b7e7"
   },
   "id": "a6c2ddd45323b7e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 로지스틱 회귀\n",
    "로지스텍 모델(logistic model), 로짓 모델(logit model)은 로그-오즈(log-odds) 사건을 선형조합을 통해 예측하는 것을 의미합니다. 로지스틱 회귀(logistic regression)는 이에 대한 매개변수를 예측하기 위한 알고리즘입니다.\n",
    "이진 회귀 분석(binary logistic regression)에선 지시변수(indicator variable)를 독립변수로 해독해 사용합니다—이때, 지시변수는 0과 1로 주어지며 독립변수는 결국 0 ~ 1의 연속변수(continuous variable)가 됩니다.\n",
    "- 두 개 이상의 이진변수를 독립변수로 사용하는 경우엔 다중 회귀 분석(multinomial logistic regression)라 합니다. 이때, 다수의 이진변수는 곧 범주형 변수(categorical variable)로 다시 정형화될 수 있습니다—이때, 각 범주들이 정렬되었고 이들의 연속적 특징이 의미를 가질 때, 정렬적 로지스틱 회귀(ordinal logistic regression)라 합니다.\n",
    "- 로지스틱 회귀 자체는 단순한 확률적 예측을 제공할 뿐이지만 분류기로써도 사용될 수 있으며 이러한 형태를 통계적 분류기(statical classifier)라고 합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d4f79291fae8d73"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "  def __init__(self, dataset, indices, transform=None, encoder=None):\n",
    "    self.dataset, self.indices = dataset, indices\n",
    "    self.transform, self.encoder = transform, encoder\n",
    "  def __getitem__(self, item: int):\n",
    "    idx = self.indices[item]\n",
    "    feature, label = self.dataset.data[idx], self.dataset.target[idx]\n",
    "    if self.transform: feature = self.transform(feature)\n",
    "    if self.encoder: label = self.encoder(label)\n",
    "    return feature, label\n",
    "  def __len__(self): return len(self.indices)"
   ],
   "metadata": {
    "id": "70149775527c0926",
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:09.080194300Z",
     "start_time": "2025-03-02T03:46:08.949858400Z"
    }
   },
   "id": "70149775527c0926"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "indices = random.sample(range(cancer_set.data.__len__()), 100)\n",
    "\n",
    "# init Datasets\n",
    "support_set = Dataset(cancer_set, indices[:50])\n",
    "query_set = Dataset(cancer_set, indices[50:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T03:46:09.083198500Z",
     "start_time": "2025-03-02T03:46:08.962502400Z"
    }
   },
   "id": "86d0143c5032166d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 손실함수 정의하기\n",
    "손실함수(loss function)는 기계학습에서 모델의 예측값과 실제값 사이의 차이를 측정하는 함수입니다. 손실 함수는 모델이 얼마나 잘 수행되고 있는지를 수치적으로 나타내며, 이 값을 최소화하는 것이 모델 학습의 목표입니다.\n",
    "* $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "* $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "> 전체적 관찰에 대한 오차를 위한 함수는 비용함수(cost function)라고 별칭합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2147ee2fd53ec698"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# define loss functions(MSE, MAE)\n",
    "def mean_squared_error(independent, dependent, weight):\n",
    "  probability = np.dot(independent, weight)\n",
    "  return np.mean((probability - dependent) ** 2)\n",
    "# mean_squared_error\n",
    "\n",
    "def mean_absolute_error(independent, dependent, weight):\n",
    "  probability = np.dot(independent, weight)\n",
    "  return np.mean(abs(probability - dependent))\n",
    "# mean_absolute_error"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T05:32:19.148187700Z",
     "start_time": "2025-03-02T05:32:19.144528300Z"
    }
   },
   "id": "e4f58cfa6f4638e2"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1 / (1 + np.exp(-1 * np.clip(x, -1e2, 1e2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T05:32:22.339047900Z",
     "start_time": "2025-03-02T05:32:22.333625100Z"
    }
   },
   "id": "8e431387"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 수식화\n",
    "$\\left\\{ y_i, x_{i1}, …, x_{ip} \\right\\}^n_{i=1}$를 통해 선형모델은 독립변수 y와 종속변수 $\\beta$를 예측해야 합니다.\n",
    "$$y_{i} = \\beta_{0} + \\beta_{1}x_{i1} + ... + \\beta_{p}x_{ip} + \\epsilon_{i} = \\beta_{p}x_{ip} + \\epsilon_{i}$$\n",
    "* $y$는 관측값의 벡터입니다.\n",
    "* $x$는 열 벡터 $x_i$의 행렬 또는 다차원의 행 벡터 $x_j$입니다.\n",
    "* $\\beta$는 $p+1$ 차원의 매개변수 벡터입니다.\n",
    "* $\\epsilon$는 $\\epsilon_i$의 벡터입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9bc767ae9a40322"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "  def __init__(self, n_inpt): self.weight = np.zeros(shape=(n_inpt))\n",
    "  def gdr(self, x, y, lr):\n",
    "    indications = self.forward(x)\n",
    "    self.weight -= (lr / x.shape[0]) * np.dot(x.T, (indications - y))\n",
    "  # gdr\n",
    "  def train(self, dataset, iters: int, lr=0.01):\n",
    "    for _ in range(iters):\n",
    "      for feature, label in dataset: self.gdr(feature, label, lr=lr)\n",
    "  # train\n",
    "  def forward(self, x): return sigmoid(np.dot(x, self.weight))\n",
    "# LogisticRegression"
   ],
   "metadata": {
    "id": "9ac7d18d7d81de70",
    "ExecuteTime": {
     "end_time": "2025-03-02T05:32:33.208925Z",
     "start_time": "2025-03-02T05:32:33.204708300Z"
    }
   },
   "id": "9ac7d18d7d81de70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 경사 하강법\n",
    "경사하강법(Gradient Descent)은 머신러닝과 최적화 분야에서 널리 사용되는 알고리즘으로, 함수의 최솟값을 찾기 위해 반복적으로 파라미터를 업데이트하는 방법입니다. 주로 모델의 손실을 최소화하는 데 사용되며, 이 과정에서 모델의 가중치를 조정합니다.\n",
    "1. 가중치 $\\theta$를 임의의 값을 초기화합니다.\n",
    "2. 현재의 가중치 $\\theta$에서 손실함수의 경사 $\\nabla{f(\\theta)}$를 계산합니다.\n",
    "3. 가중치 갱신하기: $\\theta_{\\text{new}} \\leftarrow \\theta - \\mathcal{n}\\cdot\\nabla{f(\\theta)}$\n",
    "4. 경사가 충분히 작아질 때까지 2 ~ 3을 반복합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a47c002b9ebfde24"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def GDR(model, lr):\n",
    "  def _GDR(x, y):\n",
    "    pred = model.forward(x)\n",
    "    model.weight -= lr * np.dot(x.T, (pred - y))\n",
    "  return _GDR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T05:32:33.675460600Z",
     "start_time": "2025-03-02T05:32:33.666923600Z"
    }
   },
   "id": "8c53a21fd55ef77d"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2becd52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2becd52",
    "outputId": "92b1e3ad-635d-4d1f-b3c2-e8f963ddea59",
    "ExecuteTime": {
     "end_time": "2025-03-02T05:32:34.313789600Z",
     "start_time": "2025-03-02T05:32:34.053577Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 391.40it/s, loss=1.66e+6]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(100))\n",
    "\n",
    "# init and train a model\n",
    "model = LogisticRegression(30)\n",
    "optimizer = GDR(model, 0.001)\n",
    "for _ in progress_bar:\n",
    "  loss = 0.\n",
    "  for feature, label in support_set:\n",
    "    optimizer(feature, label)\n",
    "    loss += mean_squared_error(feature, label, model.weight)\n",
    "  progress_bar.set_postfix(loss=loss/len(support_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 검증하기\n",
    "아래 검증 결과를 통해 해당 구현이 `0.92`의 정확도를 달성했음을 알 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80f4f7fc3134b36e"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f4b9b467cabc899",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "4f4b9b467cabc899",
    "outputId": "5432aa6b-fd41-487b-e3d3-11bfbc4ee991",
    "ExecuteTime": {
     "end_time": "2025-03-02T05:32:35.029459900Z",
     "start_time": "2025-03-02T05:32:35.012428700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.94(47/50)\n"
     ]
    }
   ],
   "source": [
    "count, n_samples = 0, len(query_set)\n",
    "for feature, label in support_set:\n",
    "  pred = model.forward(feature)\n",
    "  if round(pred) == label: count += 1\n",
    "print(f\"accuracy: {count / n_samples:.2f}({count}/{n_samples})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
