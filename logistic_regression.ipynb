{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62db0eaa",
   "metadata": {
    "id": "62db0eaa",
    "ExecuteTime": {
     "end_time": "2025-06-13T14:05:18.924734100Z",
     "start_time": "2025-06-13T14:05:18.893257700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[data sepecification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "a6c2ddd45323b7e7"
   },
   "id": "a6c2ddd45323b7e7"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "cancer_set = load_breast_cancer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:05:18.937624700Z",
     "start_time": "2025-06-13T14:05:18.898522500Z"
    }
   },
   "id": "e66a9cf95b568f9a"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "  def __init__(self, dataset, indices, transform=None, encoder=None):\n",
    "    self.dataset, self.indices = dataset, indices\n",
    "    self.transform, self.encoder = transform, encoder\n",
    "  def __getitem__(self, item: int):\n",
    "    idx = self.indices[item]\n",
    "    feature, label = self.dataset.data[idx], self.dataset.target[idx]\n",
    "    if self.transform: feature = self.transform(feature)\n",
    "    if self.encoder: label = self.encoder(label)\n",
    "    return feature, label\n",
    "  def __len__(self): return len(self.indices)"
   ],
   "metadata": {
    "id": "70149775527c0926",
    "ExecuteTime": {
     "end_time": "2025-06-13T14:05:18.958700700Z",
     "start_time": "2025-06-13T14:05:18.909679200Z"
    }
   },
   "id": "70149775527c0926"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "indices = random.sample(range(cancer_set.data.__len__()), 100)\n",
    "\n",
    "# init Datasets\n",
    "support_set = Dataset(cancer_set, indices[:50])\n",
    "query_set = Dataset(cancer_set, indices[50:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:05:18.960214300Z",
     "start_time": "2025-06-13T14:05:18.913209Z"
    }
   },
   "id": "86d0143c5032166d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "**Logistic Model (or Logit Model)** is a statistical method that predicts the log-odds of an event using a linear combination of variables. The most common measurement is **Cross-Entropy Loss ( or Log Loss)**, which differs from linear least squares but can still be explained as ordinary least squares."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d4f79291fae8d73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Sigmoid/Logistic Function as an Activation Function\n",
    "**Activation Function** is a mathematical function applied to the output. Its main purposes are adding non-linearity to the model and leveraging the output range to help make better decisions—most image recognition and NLP models cannot work without it.\n",
    "$$ \\text{sigmoid function} = \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "- Although it introduces non-linearity to models, the activation function must be differentiable to calculate gradients.\n",
    "- **Decision Boundary** is where the model changes its prediction. There are several types:\n",
    "  - A point for a single feature $x$\n",
    "  - A line for two features $x$\n",
    "  - Hyperplane for higher dimensions $x$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80e34a4049e5259d"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1 / (1 + np.exp(-1 * np.clip(x, -1e2, 1e2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:05:18.960214300Z",
     "start_time": "2025-06-13T14:05:18.919278700Z"
    }
   },
   "id": "8e431387"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Formulation\n",
    "$$h(x_i) = \\sigma(z =\\theta_0 + \\theta_1 \\cdot x_{(i,1)} + ... + \\theta_n \\cdot x_{(n, i)} + \\epsilon_i) \\quad\\text{where is } \\sigma(x) = \\frac{1}{1 + e^{-z}}$$\n",
    "- Input $x$ is called feature vector while output $h(x)$ is called label.\n",
    "- $z$ represents the linear combination of inputs and weights\n",
    "- while $z$ can be any real number, $\\sigma$ (called **Sigmoid Function**) maps it to a probability space between $(0, 1)$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c98f830636e3e626"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "  def __init__(self, n_inpt): self.weight = np.zeros(shape=(n_inpt))\n",
    "  def forward(self, x): return sigmoid(np.dot(x, self.weight))\n",
    "# LogisticRegression"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:05:18.961213100Z",
     "start_time": "2025-06-13T14:05:18.924734100Z"
    }
   },
   "id": "9ac7d18d7d81de70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross-entropy/Log Loss\n",
    "**Cross-Entropy** **Loss** is an algorithm that fits or evaluates the parameters $\\theta$ as log-likelihood, which differs slightly from least squares. It ensures convexity during gradient descent and penalizes wrong predictions more heavily when the model is *\"confident but wrong\"*.\n",
    "$$ J(\\theta) = =\\frac{1}{m}\\sum^{m}_{i=1}{y^{(i)}\\log{h(x^{i})} + (1 -y^i)\\log{(1 - h(x^i))}} $$\n",
    "- To minimize $J(\\theta)$, update weights using the gradient: $\\theta_j := \\theta_j - \\alpha\\frac{\\Delta{J(\\theta)}}{\\Delta{\\theta_j}}$\n",
    "    - Where the gradient is: $\\frac{\\Delta{J(\\theta)}}{\\Delta{\\theta_j}} = \\frac{1}{m} \\cdot \\sum^{m}_{i=1}{h_(x^i) - y^i} \\cdot x_k^i$\n",
    "- Vectorized update rule from the above: $\\theta := \\theta - \\frac{\\alpha}{m} \\cdot x \\cdot (h(x) - y)$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccde647048a19466"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# define loss functions(MSE, MAE)\n",
    "def cross_entropy_loss(independent, dependent, weight):\n",
    "  probability = np.dot(independent, weight)\n",
    "  return np.mean(dependent * np.log(np.clip(probability, 1e-6, 1.0)) + (1 - dependent) * np.log(np.clip(1 - probability, 1e-6, 1.0)))\n",
    "# cross_entropy_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:12:18.064638100Z",
     "start_time": "2025-06-13T14:12:18.055328600Z"
    }
   },
   "id": "a15fa8568755a8bd"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def update_rule(model, lr):\n",
    "  def _update_rule(x, y):\n",
    "    pred = model.forward(x)\n",
    "    model.weight -= (lr / len(x)) * np.dot(x.T, (pred - y))\n",
    "  return _update_rule"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:12:18.335405800Z",
     "start_time": "2025-06-13T14:12:18.329388300Z"
    }
   },
   "id": "8c53a21fd55ef77d"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression: 100%|██████████| 100/100 [00:00<00:00, 323.57epoch/s, loss=-0.276]\n"
     ]
    }
   ],
   "source": [
    "# init and train a model\n",
    "model = LogisticRegression(30)\n",
    "optimizer = update_rule(model, 0.001)\n",
    "\n",
    "progress_bar = tqdm(range(100), desc=\"Training Logistic Regression\", unit=\"epoch\", leave=True, dynamic_ncols=True)\n",
    "for _ in progress_bar:\n",
    "  loss = 0.\n",
    "  for feature, label in support_set:\n",
    "    optimizer(feature, label)\n",
    "    loss += cross_entropy_loss(feature, label, model.weight)\n",
    "  progress_bar.set_postfix(loss=loss/len(support_set))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:12:19.023132800Z",
     "start_time": "2025-06-13T14:12:18.705516100Z"
    }
   },
   "id": "f2becd52"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f4b9b467cabc899",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "4f4b9b467cabc899",
    "outputId": "5432aa6b-fd41-487b-e3d3-11bfbc4ee991",
    "ExecuteTime": {
     "end_time": "2025-06-13T14:12:05.349879500Z",
     "start_time": "2025-06-13T14:12:05.343169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.82(41/50)\n"
     ]
    }
   ],
   "source": [
    "count, n_samples = 0, len(query_set)\n",
    "for feature, label in support_set:\n",
    "  pred = model.forward(feature)\n",
    "  if round(pred) == label: count += 1\n",
    "print(f\"accuracy: {count / n_samples:.2f}({count}/{n_samples})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
