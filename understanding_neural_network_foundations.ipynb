{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c34f6de",
   "metadata": {
    "id": "2c34f6de",
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:18.135702900Z",
     "start_time": "2025-06-14T08:41:18.042789900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4bbc91d5",
   "metadata": {
    "id": "4bbc91d5",
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:18.161030700Z",
     "start_time": "2025-06-14T08:41:18.051650200Z"
    }
   },
   "outputs": [],
   "source": [
    "iris_set = load_iris()\n",
    "n_classes = iris_set.target_names.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[dataset specification](https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_iris.html)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc18e34b80be31e6"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "  def __init__(self, dataset, indices, transform=None, encoder=None):\n",
    "    self.dataset, self.indices = dataset, indices\n",
    "    self.transform, self.encoder = transform, encoder\n",
    "  def __getitem__(self, item: int):\n",
    "    assert not item > len(self.indices), \"the index is out of bound\"\n",
    "    idx = self.indices[item]\n",
    "    feature, label = self.dataset.data[idx], self.dataset.target[idx]\n",
    "    if self.transform: feature = self.transform(feature)\n",
    "    if self.encoder: label = self.encoder(label)\n",
    "    return feature, label\n",
    "  def __len__(self): return len(self.indices)"
   ],
   "metadata": {
    "id": "9cb5eee78991a95",
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:18.375125500Z",
     "start_time": "2025-06-14T08:41:18.067365500Z"
    }
   },
   "id": "9cb5eee78991a95"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "indices = random.sample(range(iris_set.data.__len__()), 100)\n",
    "encoder = lambda index: np.eye(n_classes)[index]\n",
    "\n",
    "# init Datasets\n",
    "trainset = Dataset(iris_set, indices[:50], encoder=encoder)\n",
    "testset = Dataset(iris_set, indices[50:], encoder=encoder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:18.376713800Z",
     "start_time": "2025-06-14T08:41:18.076413300Z"
    }
   },
   "id": "63380a0f9ca238ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Understanding Neural Network Foundations: Perceptron, ADALINE and MLP\n",
    "Understanding Neural Network Foundations: Perceptron, ADALINE and MLP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "837d54166f29f95f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perceptron\n",
    "\n",
    "**Perceptron (or Binary Classifier)** is an algorithm that classifies data into two categories by determining which class an input belongs to. Unlike regression models which focus on prediction and analysis, the perceptron specializes in classifying inputs into binary classes represented as $0, 1$.\n",
    "\n",
    "Perceptron was invented by Warren McCulloch and Walter Pitts in 1943. Rather than software, it was implemented as a machine—the IBM 704—designed to classify images. Later, the Mark 1 Perceptron emerged as the software version of this system."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b4aa95a1b8c335c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Formulation and Learning Rule\n",
    "It does not use partial derivatives, but instead relies on the simple linear separability of the data.\n",
    "\n",
    "$$\n",
    "f(z) = f[w(t) \\cdot x_j]\n",
    "$$\n",
    "\n",
    "1. Initialize weights to $0$ or a random number.\n",
    "2. Calculate outputs: $y_j(t) = f[w(t) \\cdot x_j]$\n",
    "3.  Update weights: $w_i(t + 1) = w_i(t) + r \\cdot (d_j - y_j(t)) \\cdot x_{j, i}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d37598dc8221fcea"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "  def __init__(self, n_inpt, n_ouput):\n",
    "    self.n_ouput, self.n_inpt = n_ouput, n_inpt\n",
    "    self.weight = np.zeros(shape=(self.n_inpt, self.n_ouput))\n",
    "  # __init__\n",
    "\n",
    "  def forward(self, x): return np.dot(x.T, self.weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:18.407467100Z",
     "start_time": "2025-06-14T08:41:18.089243700Z"
    }
   },
   "id": "837296d2380cee67"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def hinge_loss(y, o):\n",
    "  return np.sum(np.maximum(0, -1 * (y - o)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:18.435682200Z",
     "start_time": "2025-06-14T08:41:18.095770200Z"
    }
   },
   "id": "4975ebba4b6ea193"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def GDR(model, lr):\n",
    "  def _GDR(x, y):\n",
    "    pred = model.forward(x)\n",
    "    error = y - pred\n",
    "    grads = np.dot(x.reshape(1, -1).T, error.reshape(1, -1))\n",
    "    model.weight += grads * lr\n",
    "  return _GDR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:18.466445700Z",
     "start_time": "2025-06-14T08:41:18.108661Z"
    }
   },
   "id": "e727095e5abf19fc"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Perceptron: 100%|██████████| 1000/1000 [00:05<00:00, 167.59epoch/s, loss=15.33]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(1000), desc=\"Training Perceptron\", unit=\"epoch\", leave=True, dynamic_ncols=True)\n",
    "\n",
    "# init and train a model\n",
    "model = Perceptron(4, 3)\n",
    "optimizer = GDR(model, 0.001)\n",
    "for _ in progress_bar:\n",
    "  loss = 0.\n",
    "  for feature, label in trainset:\n",
    "    optimizer(feature, label)\n",
    "    loss += hinge_loss(model.forward(feature), label)\n",
    "  progress_bar.set_postfix({\"loss\": f\"{loss:.2f}\"})\n",
    "# for for"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:24.118196200Z",
     "start_time": "2025-06-14T08:41:18.120154600Z"
    }
   },
   "id": "13d9bb9b20ea63aa"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.84(42/50)\n"
     ]
    }
   ],
   "source": [
    "count, n_samples = 0, len(testset)\n",
    "for feature, label in testset:\n",
    "  pred = model.forward(feature)\n",
    "  if np.argmax(pred) == np.argmax(label): count += 1\n",
    "print(f\"accuracy: {count / n_samples:.2f}({count}/{n_samples})\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:24.123943900Z",
     "start_time": "2025-06-14T08:41:24.106443600Z"
    }
   },
   "id": "8d37ff49ba82ff1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing Perceptron and Logistic Regression\n",
    "The perceptron is often confused with logistic regression. Though they share similarities, they are entirely different concepts.\n",
    "\n",
    "- Perceptron outputs a hard class label based on a threshold, while logistic regression outputs a probability using the sigmoid function.\n",
    "- Logistic regression is a probabilistic model, while the perceptron is deterministic.\n",
    "- The perceptron uses hinge loss instead of gradient descent rules and cross-entropy.\n",
    "- The perceptron converges only when data is linearly separable, while logistic regression always converges."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a521f68033c7bae8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ADALINE\n",
    "**ADALINE (Adaptive Linear Neuron, Adaptive Linear Element)** is an enhanced version of the perceptron. It classifies data using a layer of parallel perceptrons. This structure serves as a prototype for artificial neural networks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91ee24633e08756"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Formulation and Learning Rule\n",
    "It takes multiple inputs and produce a single output as a multi-layer neural network composed of various nodes.\n",
    "\n",
    "$$\n",
    "y = \\sum^{n}_{j=0}x_jw_j + \\theta\n",
    "$$\n",
    "- $x$ represents the input vector while $x_0 = 1$ is bias.\n",
    "- $y$ represents the model's output.\n",
    "- $w$ represents the weights, where $w_0 = 0$ is used for local bias.\n",
    "- $n$ is the number of inputs in the dataset.\n",
    "- $\\theta$ represents the global bias constant.\n",
    "\n",
    "**Learning Rule:**\n",
    "- The least mean square error is calculated as $E = (o - y)^2$.\n",
    "- Update the weights: $w \\leftarrow w + \\eta(o-y) \\cdot x$.\n",
    "    - $\\eta$ represents the learning rate.\n",
    "    - $o$ represents the target output value.\n",
    "\n",
    "MADALINE (Many ADALINE), a variant of ADALINE, uses a structure that connects three ADALINE units linearly. It is similar to modern neural networks but differs in that it uses different functions per layer, making backpropagation impossible."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53a79f56010f395f"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class ADALINE:\n",
    "  def __init__(self, n_inpt, n_ouput):\n",
    "    self.n_ouput, self.n_inpt = n_ouput, n_inpt\n",
    "    self.weight = np.zeros(shape=(self.n_inpt, self.n_ouput))\n",
    "  # __init__\n",
    "\n",
    "  def forward(self, x): return relu(np.dot(x.T, self.weight))\n",
    "# ADALINE"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:24.209386100Z",
     "start_time": "2025-06-14T08:41:24.119432600Z"
    }
   },
   "id": "4bd9b1f0"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1 / (1 + np.exp(-1 * x))\n",
    "def relu(x): return np.maximum(0, x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:24.213567300Z",
     "start_time": "2025-06-14T08:41:24.131205700Z"
    }
   },
   "id": "4ed23d9d"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 659.96it/s]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(1000))\n",
    "\n",
    "# init and train a model\n",
    "model = ADALINE(4, 3)\n",
    "optimizer = GDR(model, 0.001)\n",
    "for _ in progress_bar:\n",
    "  for feature, label in trainset:\n",
    "    optimizer(feature, label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:25.683092200Z",
     "start_time": "2025-06-14T08:41:24.141517800Z"
    }
   },
   "id": "2a7f77f00d7b2c8e"
  },
  {
   "cell_type": "code",
   "source": [
    "count, n_samples = 0, len(testset)\n",
    "for feature, label in testset:\n",
    "  pred = model.forward(feature)\n",
    "  if np.argmax(pred) == np.argmax(label): count += 1\n",
    "print(f\"accuracy: {count / n_samples:.2f}({count}/{n_samples})\")"
   ],
   "metadata": {
    "id": "bVsrTBzBRskr",
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:25.687751300Z",
     "start_time": "2025-06-14T08:41:25.676686300Z"
    }
   },
   "id": "bVsrTBzBRskr",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.92(46/50)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP, Multi-Layer Perceptron\n",
    "\n",
    "**Multi-Layer Perceptron ( or MLP, fully connected artificial neural network)** consists of three layers of perceptrons with non-linear activation functions. Its primary purpose is to classify inputs in a linear manner.\n",
    "\n",
    "- Every perceptron in an MLP is fully connected to the next layer of perceptrons, giving each perceptron multidimensional weights $w_{i,j}$.\n",
    "- These perceptrons function as signal processing units, similar to neurons in the human brain, which is why they are called **neurons**.\n",
    "- Every neuron has an activation function that maps scalar responses to a non-linear number range. This is a crucial concept in MLPs that enables their functionality and improves their performance.\n",
    "    - In most implementations, MLPs use the hyperbolic tangent or sigmoid function as their activation function.\n",
    "\n",
    "It is considered a very prototype of modern neural networks and is sometimes called a “vanilla neural network”, though it differs in its use of forward propagation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97cb4680a8ae2029"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Formulation and Learning Rule\n",
    "\n",
    "The learning rule is based on the concept of a neuron; it updates the neuron's weights by calculating partial derivatives of their cost/loss function. This represents a fundamental mechanism of modern deep learning.\n",
    "\n",
    "$$\n",
    "w_j(l)(n) = \\sum_{i} w_{ji}(l)(n)⋅y_i(l−1)(n)+b_j(l)(n)y_j(l)(n)=\\phi\\large(w_j(l)(n)\\large)y_j(l)(n)=\\phi \\large(w_j(l)(n)\\large)\n",
    "$$\n",
    "\n",
    "- Calculate the error $\\epsilon$ at the output layer using $\\epsilon_j(n) = d_j(n) - y_j(n)$, where $d_j(n)$ represents the desired output values of the model.\n",
    "    - $\\therefore e(n) = \\frac{1}{2} \\cdot \\sum\n",
    "    {e_j^2(n)}$\n",
    "- Update using the gradient descent rule: $\\Delta w_{ji}(n) = \\eta \\cdot \n",
    "\\frac{\\Delta \\epsilon(n)}{\\Delta w_{j}(n)} \\cdot y_i(n)$.\n",
    "    - $e_j$ represents the output of layer j.\n",
    "    - $y_i(n)$ represents the output of neuron i in the previous layer.\n",
    "    - $w_i$ represents the weights of neuron i, while $\\eta$ is the learning rate.\n",
    "    - $\\frac{\\Delta \\epsilon(n)}{\\Delta w_i(n)}$ is the partial derivative of $\\epsilon$ with respect to $w_i$, which can be expressed as: -$\\frac{d\\epsilon(n)}{dv_j(n)}=e_j(n)\\phi'(w_j(n)) = \\phi'(w_j(n)) \\cdot \\sum_{k}-\\frac{d\\epsilon(n)}{dv_k(n)}w_{kj}(n)$ where $\\phi$ represents the activation function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d46e4d65305dcea"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class MLP:\n",
    "  def __init__(self, n_inpt, n_ouput, n_hidden=10):\n",
    "    self.n_ouput, self.n_inpt = n_ouput, n_inpt\n",
    "    self.weight1 = np.zeros(shape=(self.n_inpt, n_hidden))\n",
    "    self.weight2 = np.zeros(shape=(n_hidden, self.n_ouput))\n",
    "  # __init__\n",
    "\n",
    "  def forward(self, x):\n",
    "    hidden = relu(np.dot(x.T, self.weight1))\n",
    "    return sigmoid(np.dot(hidden.T, self.weight2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:25.733941500Z",
     "start_time": "2025-06-14T08:41:25.695668500Z"
    }
   },
   "id": "3e2d94dc8da3c8d8"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def GDR(model, lr):\n",
    "  def _GDR(x, y):\n",
    "    hidden = relu(np.dot(x.T, model.weight1))\n",
    "    pred = sigmoid(np.dot(hidden.T, model.weight2))\n",
    "\n",
    "    error = y - pred\n",
    "    grads2 = np.dot(hidden.reshape(1, -1).T, error.reshape(1, -1))\n",
    "    model.weight2 += grads2 * lr\n",
    "\n",
    "    error_hidden = np.dot(error, model.weight2.T) * (hidden > 0)\n",
    "    grads1 = np.dot(x.reshape(1, -1).T, error_hidden.reshape(1, -1))\n",
    "    model.weight1 += grads1 * lr\n",
    "  return _GDR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:25.761597700Z",
     "start_time": "2025-06-14T08:41:25.737270800Z"
    }
   },
   "id": "4938938cb9bb488"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MLP: 100%|██████████| 500/500 [00:04<00:00, 107.94it/s, loss=25.00]\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(500), desc=\"Training MLP\", leave=True, dynamic_ncols=True)\n",
    "\n",
    "# init and train a model\n",
    "model = MLP(4, 3, n_hidden=12)\n",
    "optimizer = GDR(model, 0.001)\n",
    "for _ in progress_bar:\n",
    "  loss = 0.\n",
    "  for feature, label in trainset:\n",
    "    optimizer(feature, label)\n",
    "    loss += hinge_loss(model.forward(feature), label)\n",
    "  progress_bar.set_postfix(loss=f\"{loss.item():.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:30.401509600Z",
     "start_time": "2025-06-14T08:41:25.747430600Z"
    }
   },
   "id": "7ce9acd9cb0c4426"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.42(21/50)\n"
     ]
    }
   ],
   "source": [
    "count, n_samples = 0, len(testset)\n",
    "\n",
    "for feature, label in testset:\n",
    "    pred = model.forward(feature)\n",
    "    if np.argmax(pred) == np.argmax(label): count += 1\n",
    "print(f\"accuracy: {count / n_samples:.2f}({count}/{n_samples})\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-14T08:41:30.406219200Z",
     "start_time": "2025-06-14T08:41:30.395622400Z"
    }
   },
   "id": "4167e1901ba07ef3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
